#!/usr/bin/env python3
"""
æµå¼ ASR æ¼”ç¤ºè„šæœ¬

åŠŸèƒ½ï¼š
- å®æ—¶æ˜¾ç¤ºä¸­é—´ç»“æœï¼ˆğŸ“ Partialï¼‰
- æ˜¾ç¤ºæœ€ç»ˆç»“æœï¼ˆâœ… Finalï¼‰
- æ”¯æŒ VAD è¯­éŸ³è¾¹ç•Œæ£€æµ‹
- 3 æºåˆ†ç¦» + è¯´è¯äººéªŒè¯

ç”¨æ³•ï¼š
    python demo_streaming_asr.py --input-wav mix.wav --target-wav target.wav [options]
"""
import sys
import os
import time
import argparse

# æ·»åŠ é¡¹ç›®è·¯å¾„
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(script_dir))
sys.path.insert(0, project_root)
sys.path.insert(0, os.path.dirname(script_dir))

import numpy as np
import torchaudio

# ANSI é¢œè‰²
RESET = "\033[0m"
BOLD = "\033[1m"
BLUE = "\033[94m"
GREEN = "\033[92m"
YELLOW = "\033[93m"
CYAN = "\033[96m"
GRAY = "\033[90m"


def parse_args():
    p = argparse.ArgumentParser(description="Streaming ASR Demo with Partial/Final")

    # è¾“å…¥
    p.add_argument("--input-wav", required=True, help="Mixed audio file")
    p.add_argument("--target-wav", required=True, help="Target speaker reference")

    # æ¨¡å‹è·¯å¾„
    p.add_argument("--vad-model", required=True, help="VAD model path")
    p.add_argument("--spk-embed-model", required=True, help="Speaker embedding model")
    p.add_argument("--sense-voice", required=True, help="SenseVoice model dir")
    p.add_argument("--tokens", required=True, help="Tokens file for ASR")

    # å¤„ç†å‚æ•°
    p.add_argument("--provider", default="cpu", choices=["cpu", "cuda", "coreml"])
    p.add_argument(
        "--sv-threshold", type=float, default=0.5, help="Speaker verification threshold"
    )
    p.add_argument(
        "--chunk-duration", type=float, default=0.3, help="Audio chunk duration (s)"
    )
    p.add_argument(
        "--max-segment-duration",
        type=float,
        default=3.0,
        help="Max segment duration (s)",
    )
    p.add_argument(
        "--partial-interval",
        type=float,
        default=0.5,
        help="Partial result interval (s)",
    )
    p.add_argument(
        "--num-threads", type=int, default=2, help="Number of threads for inference"
    )

    # åˆ†ç¦»å™¨å‚æ•°
    p.add_argument("--sep-backend", default="asteroid", help="Separation backend")
    p.add_argument("--sep-checkpoint", default=None, help="Separation checkpoint path")

    # å…¶ä»–
    p.add_argument("--debug", action="store_true", help="Enable debug output")
    p.add_argument(
        "--inline", action="store_true", help="Use inline update for partial results"
    )

    return p.parse_args()


def format_result(result, use_inline=False):
    """æ ¼å¼åŒ–ç»“æœæ˜¾ç¤º

    Args:
        result: StreamingResult
        use_inline: æ˜¯å¦ä½¿ç”¨å†…è”æ›´æ–°ï¼ˆpartial è¦†ç›–æ˜¾ç¤ºï¼‰
    """
    is_final = result.is_final
    result_type_str = "âœ… Final" if is_final else "ğŸ“ Partial"
    color = GREEN if is_final else YELLOW

    time_str = f"{result.start_time:.2f}s-{result.end_time:.2f}s"
    sv_str = f"SV:{result.sv_score:.3f}" if result.sv_score else ""

    output = (
        f"{color}{result_type_str}{RESET} "
        f"[{CYAN}SEQ#{result.seq_id}{RESET}] "
        f"[{GRAY}{time_str}{RESET}] "
        f"[{BLUE}Stream{result.stream_id}{RESET}] "
        f"{sv_str} "
        f"{BOLD}{result.text}{RESET}"
    )

    if use_inline and not is_final:
        # ä½¿ç”¨å›è½¦ç¬¦è¦†ç›–å½“å‰è¡Œï¼ˆç”¨äºå®æ—¶æ›´æ–°æ•ˆæœï¼‰
        return f"\r{output}\033[K"  # \033[K æ¸…é™¤è¡Œå°¾

    return output


def print_result(result, use_inline=False):
    """æ‰“å°ç»“æœ"""
    formatted = format_result(result, use_inline)
    if use_inline and not result.is_final:
        print(formatted, end="", flush=True)
    else:
        if use_inline:
            print()  # æ¢è¡Œï¼ˆå› ä¸º partial å¯èƒ½åœ¨åŒä¸€è¡Œï¼‰
        print(formatted)


def main():
    args = parse_args()

    print(f"\n{BOLD}=== Streaming ASR Demo (Partial/Final) ==={RESET}\n")

    # å¯¼å…¥ pipeline
    from scripts.osd.streaming_asr_pipeline import StreamingASRPipeline, G_SAMPLE_RATE

    # åŠ è½½è¾“å…¥éŸ³é¢‘
    print(f"Loading audio: {args.input_wav}")
    waveform, sr = torchaudio.load(args.input_wav)

    if sr != G_SAMPLE_RATE:
        waveform = torchaudio.functional.resample(waveform, sr, G_SAMPLE_RATE)
        sr = G_SAMPLE_RATE

    audio = waveform[0].numpy()  # å–ç¬¬ä¸€ä¸ªé€šé“
    audio_duration = len(audio) / sr

    print(f"Audio duration: {audio_duration:.2f}s @ {sr}Hz")
    print(f"Chunk duration: {args.chunk_duration}s")
    print(f"Max segment: {args.max_segment_duration}s")
    print(f"Partial interval: {args.partial_interval}s")
    print()

    # åˆå§‹åŒ– pipeline
    print("Initializing pipeline...")
    t0 = time.perf_counter()
    pipeline = StreamingASRPipeline(args, args.target_wav)
    init_time = time.perf_counter() - t0
    print(f"Pipeline initialized in {init_time:.2f}s\n")

    # æ¨¡æ‹Ÿæµå¼å¤„ç†
    print(f"{BOLD}--- Streaming Processing ---{RESET}\n")

    chunk_size = int(args.chunk_duration * sr)
    offset = 0
    total_results = []
    final_results = []
    use_inline = args.inline
    total_compute_time = 0.0  # åªç»Ÿè®¡æ ¸å¿ƒå¤„ç†æ—¶é—´

    while offset < len(audio):
        chunk = audio[offset : offset + chunk_size]
        offset += chunk_size

        # å¤„ç†éŸ³é¢‘å—ï¼ˆåªè®¡æ—¶æ ¸å¿ƒå¤„ç†ï¼‰
        t_chunk_start = time.perf_counter()
        results = pipeline.add_audio_data(chunk)
        t_chunk_end = time.perf_counter()
        total_compute_time += t_chunk_end - t_chunk_start

        # æ˜¾ç¤ºç»“æœï¼ˆä¸è®¡å…¥ RTFï¼‰
        for r in results:
            print_result(r, use_inline)
            total_results.append(r)
            if r.is_final:
                final_results.append(r)

    # åˆ·æ–°å‰©ä½™æ•°æ®
    if use_inline:
        print()  # ç¡®ä¿æ¢è¡Œ
    print(f"\n{GRAY}--- Flushing buffer ---{RESET}\n")

    # flush ä¹Ÿè®¡å…¥æ ¸å¿ƒå¤„ç†æ—¶é—´
    t_flush_start = time.perf_counter()
    flush_results = pipeline.flush()
    t_flush_end = time.perf_counter()
    total_compute_time += t_flush_end - t_flush_start

    # æ˜¾ç¤º flush ç»“æœï¼ˆä¸è®¡å…¥ RTFï¼‰
    for r in flush_results:
        print_result(r, use_inline)
        total_results.append(r)
        if r.is_final:
            final_results.append(r)

    # æ±‡æ€»
    print(f"\n{BOLD}=== Summary ==={RESET}\n")
    print(f"Audio duration:    {audio_duration:.2f}s")
    print(f"Compute time:      {total_compute_time:.2f}s (core processing only)")
    print(f"RTF:               {total_compute_time/audio_duration:.3f}x")
    print(f"Total results:     {len(total_results)} (Partial + Final)")
    print(f"Final segments:    {len(final_results)}")

    # æœ€ç»ˆè¯†åˆ«æ–‡æœ¬
    print(f"\n{BOLD}--- Final Transcription ---{RESET}\n")

    full_text = ""
    for r in final_results:
        full_text += r.text
        print(f"  [{r.start_time:.2f}-{r.end_time:.2f}s] {r.text}")

    print(f"\n{BOLD}Full text:{RESET} {full_text}")

    # å…³é—­
    pipeline.shutdown()
    print(f"\n{GREEN}Done!{RESET}\n")


if __name__ == "__main__":
    main()
