# æºä»£ç æ¨¡å—ï¼ˆsrc/ï¼‰

æœ¬ç›®å½•åŒ…å«é¡¹ç›®çš„æ ¸å¿ƒæºä»£ç æ¨¡å—ï¼Œå®ç°äº† ASRã€è¯´è¯äººåµŒå…¥ã€é‡å æ£€æµ‹ã€è¯­éŸ³åˆ†ç¦»ç­‰åŠŸèƒ½ã€‚

## ğŸ“ ç›®å½•ç»“æ„

```
src/
â”œâ”€â”€ model.py                  # ASR å·¥å‚ + è¯´è¯äººåµŒå…¥ç®¡ç†
â”œâ”€â”€ osd/                      # é‡å æ£€æµ‹ä¸è¯­éŸ³åˆ†ç¦»æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ osd.py               # OSDï¼ˆé‡å æ£€æµ‹ï¼‰åŒ…è£…å™¨
â”‚   â”œâ”€â”€ separation.py        # è¯­éŸ³åˆ†ç¦»ï¼ˆConv-TasNetï¼‰
â”‚   â”œâ”€â”€ dataset.py           # æ•°æ®é›†å¤„ç†ï¼ˆLibri3Mix ç­‰ï¼‰
â”‚   â””â”€â”€ streaming_asr.py     # æµå¼ ASR å®ç°
â””â”€â”€ detection/               # å…¶ä»–æ£€æµ‹æ¨¡å—ï¼ˆå¦‚è¯´è¯äººæ£€æµ‹ï¼‰
```

## ğŸ”§ æ ¸å¿ƒæ¨¡å—è¯´æ˜

### model.py - ASR å·¥å‚ä¸è¯´è¯äººåµŒå…¥

**ä¸»è¦ç±»ï¼š**

#### `ASRModel`

è‡ªåŠ¨é€‰æ‹© ASR åç«¯çš„å·¥å‚ç±»ã€‚

```python
from src.model import create_asr_model

# åˆ›å»º ASR æ¨¡å‹
asr = create_asr_model(
    model_type='sense-voice',  # æˆ– 'paraformer', 'transducer'
    model_path='/path/to/model.onnx',
    tokens_path='/path/to/tokens.txt',
    provider='cuda'  # 'cuda', 'cpu', 'coreml'
)

# æ¨ç†
result = asr.recognize('audio.wav')
print(result)  # {'text': 'è½¬å½•æ–‡æœ¬', 'confidence': 0.95}
```

**æ”¯æŒçš„ ASR åç«¯ï¼š**

- **Paraformer**ï¼šå•ä¸€æ¨¡å‹ï¼Œæ¨ç†é€Ÿåº¦å¿«ï¼Œé€‚åˆå®æ—¶åœºæ™¯
- **SenseVoice**ï¼šå¤šè¯­è¨€æ”¯æŒï¼Œé€šè¿‡ Sherpa-ONNX è¿è¡Œ
- **Transducer**ï¼šEncoder-Decoder-Joiner æ¡†æ¶ï¼Œæ”¯æŒæµå¼è¯†åˆ«

#### `SpeakerEmbeddingManager`

è¯´è¯äººåµŒå…¥æå–ä¸éªŒè¯ã€‚

```python
from src.model import SpeakerEmbeddingManager

manager = SpeakerEmbeddingManager(
    model_path='/path/to/speaker_model.onnx',
    provider='cuda'
)

# æå–åµŒå…¥
embed = manager.extract_embedding('audio.wav')
print(embed.shape)  # (192,) - L2 æ­£è§„åŒ–

# è®¡ç®—ç›¸ä¼¼åº¦
similarity = manager.compute_similarity(embed1, embed2)
print(similarity)  # 0.85 - ä½™å¼¦ç›¸ä¼¼åº¦

# éªŒè¯è¯´è¯äºº
is_match = manager.verify(embed, reference_embed, threshold=0.6)
print(is_match)  # True/False
```

### osd/ - é‡å æ£€æµ‹ä¸åˆ†ç¦»æ¨¡å—

#### osd.py - OverlapAnalyzer

ä½¿ç”¨ pyannote.audio è¿›è¡Œé‡å æ£€æµ‹ã€‚

```python
from src.osd.osd import OverlapAnalyzer

analyzer = OverlapAnalyzer(
    use_auth_token='your_huggingface_token'
)

# æ£€æµ‹é‡å 
diarization = analyzer.diarize('audio.wav')
# diarization: Annotation å¯¹è±¡ï¼ŒåŒ…å«æ‰¬å£°å™¨æ—¶é—´æˆ³å’Œæ ‡ç­¾

# æå–é‡å åŒºé—´
overlaps = analyzer.get_overlaps(diarization)
for start, end in overlaps:
    print(f"é‡å : {start:.2f}s - {end:.2f}s")
```

#### separation.py - Separator

è¯­éŸ³åˆ†ç¦»æ¨¡å—ï¼Œæ”¯æŒ Conv-TasNetã€‚

```python
from src.osd.separation import Separator
import torchaudio

separator = Separator(
    model_name='conv_tasnet',  # æˆ–è‡ªå®šä¹‰ checkpoint
    n_sources=3,               # åˆ†ç¦»æºæ•°
    sample_rate=16000,
    device='cuda'
)

# åˆ†ç¦»éŸ³é¢‘
waveform, sr = torchaudio.load('mixed.wav')
separated = separator.separate(waveform)
print(separated.shape)  # (3, 1, samples) - ä¸‰ä¸ªåˆ†ç¦»çš„æº

# ä¿å­˜åˆ†ç¦»ç»“æœ
for i, src in enumerate(separated):
    torchaudio.save(f'source_{i}.wav', src, sr)

# è¯„ä¼°åˆ†ç¦»è´¨é‡ï¼ˆå¦‚æœæœ‰å‚è€ƒï¼‰
si_sdr = separator.compute_si_sdr(separated, references)
print(f"SI-SDR: {si_sdr:.2f} dB")
```

#### dataset.py - Libri3Mix æ•°æ®é›†å¤„ç†

å¤„ç† Libri3Mix/LibriMix æ•°æ®é›†ã€‚

```python
from src.osd.dataset import Libri3MixDataset

dataset = Libri3MixDataset(
    root='/path/to/LibriMix',
    sample_rate=16000,
    num_sources=3,
    max_samples=100
)

for sample in dataset:
    mixture = sample['mixture']      # æ··åˆéŸ³é¢‘
    sources = sample['sources']      # [src1, src2, src3]
    speakers = sample['speakers']    # è¯´è¯äººIDåˆ—è¡¨

    print(f"æ··åˆæ—¶é•¿: {mixture.shape[0]/16000:.2f}s")
```

#### streaming_asr.py - StreamingASR

æµå¼ ASR å®ç°ï¼Œæ”¯æŒä¸­é—´ç»“æœå’Œæœ€ç»ˆç»“æœã€‚

```python
from src.osd.streaming_asr import StreamingASR, VADStreamingASR

# åˆ›å»ºæµå¼ ASR
asr = StreamingASR(
    model_path='/path/to/model.onnx',
    tokens_path='/path/to/tokens.txt',
    chunk_size=640,  # æ ·æœ¬æ•°
    stride=320,      # æ»‘åŠ¨æ­¥é•¿
    provider='cuda'
)

# æ¨¡æ‹ŸéŸ³é¢‘æµ
import torchaudio
waveform, sr = torchaudio.load('audio.wav')

# å¤„ç†æµ
for i in range(0, waveform.shape[1], 320):
    chunk = waveform[:, i:i+640]

    # è·å–ä¸­é—´ç»“æœï¼ˆPartialï¼‰
    partial_result = asr.process_chunk(chunk, is_final=False)
    if partial_result:
        print(f"ä¸­é—´: {partial_result['text']}")

    # è¯­éŸ³ç»“æŸæ—¶è·å–æœ€ç»ˆç»“æœ
    if i == waveform.shape[1] - 1:
        final_result = asr.process_chunk(chunk, is_final=True)
        print(f"æœ€ç»ˆ: {final_result['text']}")

# VAD ç‰ˆæœ¬ï¼ˆè‡ªåŠ¨åˆ†æ®µï¼‰
vad_asr = VADStreamingASR(
    model_path='/path/to/model.onnx',
    tokens_path='/path/to/tokens.txt',
    vad_model='silero',  # VAD æ¨¡å‹
    provider='cuda'
)

# å¤„ç†ï¼Œè‡ªåŠ¨æŒ‰ VAD åˆ†æ®µ
results = vad_asr.process_stream(waveform)
for result in results:
    print(f"{result['start']:.2f}s - {result['end']:.2f}s: {result['text']}")
```

### detection/ - æ£€æµ‹æ¨¡å—

å…¶ä»–æ£€æµ‹åŠŸèƒ½çš„å®ç°ï¼ˆå¦‚å…³é”®è¯æ£€æµ‹ã€è¯´è¯äººæ£€æµ‹ç­‰ï¼‰ã€‚

## ğŸ“Š æ•°æ®æµä¸é›†æˆ

### å…¸å‹å·¥ä½œæµ

```
è¾“å…¥éŸ³é¢‘
    â†“
[VAD] â†’ æ£€æµ‹è¯­éŸ³ç‰‡æ®µ
    â†“
[OSD] â†’ æ£€æµ‹é‡å åŒºé—´
    â†“
[åˆ†ç¦»] â†’ åˆ†ç¦»æºéŸ³é¢‘ï¼ˆé‡å éƒ¨åˆ†ï¼‰
    â†“
[è¯´è¯äººéªŒè¯] â†’ ç­›é€‰ç›®æ ‡è¯´è¯äºº
    â†“
[ASR] â†’ è½¬å½•æ–‡æœ¬
    â†“
è¾“å‡ºç»“æœï¼ˆJSON/CSVï¼‰
```

### å…³é”®å‚æ•°é…ç½®

**ASR æ¨¡å‹ï¼š**

```python
# æ”¯æŒæ¨¡å‹åˆ—è¡¨
{
    'sense-voice': '/models/asr/sense-voice.onnx',
    'paraformer': '/models/asr/paraformer.onnx',
    'transducer': '/models/asr/transducer.onnx'
}
```

**æ¨ç†åç«¯ï¼š**

```python
provider_options = {
    'cuda': ['CUDAExecutionProvider', 'CPUExecutionProvider'],
    'cpu': ['CPUExecutionProvider'],
    'coreml': ['CoreMLExecutionProvider', 'CPUExecutionProvider']
}
```

**è¯´è¯äººéªŒè¯é˜ˆå€¼ï¼š**

```python
sv_threshold = 0.6  # ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œæ¨è 0.5~0.7
```

## ğŸ§ª ä½¿ç”¨ç¤ºä¾‹

### å®Œæ•´çš„ä¸‰æºåˆ†ç¦»æµç¨‹

```python
from src.model import create_asr_model, SpeakerEmbeddingManager
from src.osd.osd import OverlapAnalyzer
from src.osd.separation import Separator
import torchaudio

# åˆå§‹åŒ–å„æ¨¡å—
osd = OverlapAnalyzer()
separator = Separator(n_sources=3, device='cuda')
asr = create_asr_model('sense-voice', provider='cuda')
spk_manager = SpeakerEmbeddingManager(provider='cuda')

# åŠ è½½éŸ³é¢‘
mixture, sr = torchaudio.load('mixed.wav')
target_audio, _ = torchaudio.load('target_spk.wav')

# æ£€æµ‹é‡å 
diarization = osd.diarize(str(mixture))
overlaps = osd.get_overlaps(diarization)

# å¤„ç†é‡å ç‰‡æ®µ
for start, end in overlaps:
    # åˆ†ç¦»
    start_idx = int(start * sr)
    end_idx = int(end * sr)
    overlap_seg = mixture[:, start_idx:end_idx]

    separated = separator.separate(overlap_seg)

    # ç›®æ ‡ç­›é€‰
    target_embed = spk_manager.extract_embedding(target_audio)

    best_src = None
    best_score = 0
    for i, src in enumerate(separated):
        src_embed = spk_manager.extract_embedding(src)
        score = spk_manager.compute_similarity(target_embed, src_embed)
        if score > best_score:
            best_score = score
            best_src = i

    # ASR
    if best_src is not None and best_score > 0.6:
        result = asr.recognize(separated[best_src])
        print(f"åˆ†ç¦»æº {best_src}: {result['text']} (SV: {best_score:.2f})")
```

## ğŸ“š ç›¸å…³èµ„æº

- [ASRModel è¯¦ç»†æ–‡æ¡£](../scripts/osd/overlap3_core.py)
- [Sherpa-ONNX æ–‡æ¡£](https://github.com/k2-fsa/sherpa-onnx)
- [pyannote.audio æ–‡æ¡£](https://github.com/pyannote/pyannote-audio)
- [Asteroid æ–‡æ¡£](https://github.com/asteroid-team/asteroid)

## ğŸ”— æ¨¡å—ä¾èµ–

```
model.py
â”œâ”€â”€ onnxruntime     # ONNX æ¨ç†
â”œâ”€â”€ sherpa-onnx     # ASR + è¯´è¯äººåµŒå…¥
â””â”€â”€ numpy/torch     # æ•°å€¼è®¡ç®—

osd/
â”œâ”€â”€ pyannote.audio  # é‡å æ£€æµ‹
â”œâ”€â”€ asteroid        # è¯­éŸ³åˆ†ç¦»
â”œâ”€â”€ torchaudio      # éŸ³é¢‘å¤„ç†
â””â”€â”€ julius          # éŸ³é¢‘è¿‡æ»¤
```

## âš™ï¸ æ€§èƒ½æç¤º

1. **æ¨¡å‹åŠ è½½ä¼˜åŒ–**

   - é¢„åŠ è½½æ¨¡å‹å¹¶é‡ç”¨ï¼Œé¿å…é‡å¤åŠ è½½
   - ä½¿ç”¨ GPU æ¨ç†åŠ é€Ÿï¼ˆprovider='cuda'ï¼‰

2. **æ‰¹å¤„ç†**

   - æµå¼ ASR æ”¯æŒæ‰¹å¤„ç†å¤šä¸ªéŸ³é¢‘å—
   - åˆ†ç¦»æ¨¡å—å¯æ‰¹å¤„ç†å¤šä¸ªæ··åˆ

3. **å†…å­˜ç®¡ç†**
   - å¤§éŸ³é¢‘æ–‡ä»¶åˆ†å—å¤„ç†
   - åŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„å¼ é‡

---

**æ›´æ–°**ï¼š2026-01-09  
**ä½œè€…**ï¼šNatsuiroGinga
