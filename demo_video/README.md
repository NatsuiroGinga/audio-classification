# 声纹识别演示视频制作指南

## 📋 演示方案：单样本实时演示

### 演示流程 (3-4 分钟)

```
┌─────────────────────────────────────────────────────┐
│ 1. 展示混合音频波形 (30秒)                          │
│    - 运行: python visualize_audio.py               │
│    - 显示: 3人同时说话的混合音频                    │
└─────────────────────────────────────────────────────┘
                        ↓
┌─────────────────────────────────────────────────────┐
│ 2. 播放音频片段 (30秒)                              │
│    - 播放混合音频 mix.wav                          │
│    - 播放目标说话人 D4_750.wav                     │
└─────────────────────────────────────────────────────┘
                        ↓
┌─────────────────────────────────────────────────────┐
│ 3. 运行实时处理 (2分钟)                             │
│    - 执行: bash demo_single.sh                     │
│    - 实时显示识别结果                               │
│    - 高亮 SV 分数                                   │
└─────────────────────────────────────────────────────┘
                        ↓
┌─────────────────────────────────────────────────────┐
│ 4. 展示结果报告 (1分钟)                             │
│    - 运行: python show_results.py                  │
│    - 美化输出表格                                   │
│    - 性能统计数据                                   │
└─────────────────────────────────────────────────────┘
```

---

## 🚀 快速开始

### 步骤 1: 生成可视化素材

```bash
# 生成所有可视化图表（波形图、对比图、频谱图）
python visualize_audio.py --sample s1 --type all

# 仅生成对比图（推荐用于演示）
python visualize_audio.py --sample s1 --type comparison
```

**输出文件**：

- `visuals/s1_comparison.png` - 混合音频与目标说话人对比图 ✨
- `visuals/s1_mix_waveform.png` - 混合音频波形图
- `visuals/s1_target_waveform.png` - 目标说话人波形图
- `visuals/s1_mix_spectrogram.png` - 混合音频频谱图
- `visuals/s1_target_spectrogram.png` - 目标说话人频谱图

---

### 步骤 2: 运行实时演示

```bash
# 运行单样本演示（s1 - 效果最好）
bash demo_single.sh
```

**演示过程**：

1. 显示系统信息
2. 加载模型（约 2 秒）
3. 实时处理音频（9.62 秒音频）
4. 输出识别结果（逐条显示）
5. 保存结果到 `demo_output/`

**关键输出示例**：

```
[clean] SV=0.669: 据为苏炳爱邓田。
[overlap] SV=0.727: 邓铁梅等。
[overlap] SV=0.785: 苏北军的一些爱国将士马占山、吕杜唐。
[clean] SV=0.842: 美等也奋起抗战。
[full_separation] SV=0.904: 据5，朱炳爱、邓铁梅等也奋起抗战。
```

---

### 步骤 3: 展示美化结果

```bash
# 使用 rich 库美化输出（适合录屏）
python show_results.py --output-dir demo_output --delay 0.5
```

**显示内容**：

- 🎤 系统标题面板
- 📊 音频信息表格
- ✨ 识别结果详细列表（带颜色和图标）
- 📈 性能统计（RTF、SV 分数、处理速度）

---

## 📹 录制建议

### 录制工具

- **屏幕录制**: OBS Studio / Screen Studio
- **分辨率**: 1920x1080 (1080p)
- **帧率**: 30fps
- **终端**: iTerm2 / Windows Terminal (带主题)

### 录制布局

```
┌────────────────────────────────────────────────────┐
│                    浏览器窗口                       │
│           显示可视化图片 (对比图)                   │
│                  (全屏或 70%)                       │
├────────────────────────────────────────────────────┤
│                                                     │
│                终端窗口 (底部)                       │
│              运行演示脚本输出                        │
│                   (30%)                            │
└────────────────────────────────────────────────────┘
```

或使用画中画：

```
┌────────────────────────────────────────────────────┐
│                                                     │
│           终端窗口 (全屏)                            │
│         运行 demo_single.sh                         │
│                                                     │
│    ┌──────────────────┐                            │
│    │   可视化图片      │ (右上角悬浮)               │
│    │   小窗口显示      │                            │
│    └──────────────────┘                            │
│                                                     │
└────────────────────────────────────────────────────┘
```

---

### 录制脚本 (旁白参考)

#### 【开场 - 展示可视化】(30 秒)

```
"大家好，今天展示一个多说话人语音分离与识别系统。
这是一段3人同时说话的混合音频（展示波形图上半部分）。
下面是我们的目标说话人音频（展示波形图下半部分）。
系统的任务是从混合音频中准确识别出这位说话人的语音。"
```

#### 【播放音频】(30 秒)

```
"首先听一下混合音频... (播放 mix.wav 前5秒)
可以听到三个人在同时说话，非常嘈杂。

这是目标说话人的音频... (播放 D4_750.wav 前5秒)
接下来看系统如何处理。"
```

#### 【运行处理】(2 分钟)

```
"现在开始实时处理... (运行 demo_single.sh)

系统加载了四个核心模块：
- pyannote.audio 检测重叠语音
- Asteroid Conv-TasNet 进行3源分离
- 3DSpeaker 提取说话人特征
- SenseVoice 完成语音识别

可以看到结果正在逐条输出...

(结果出现时)
第一条：SV分数0.669，识别为'据为苏炳爱邓田'
第二条：SV分数0.785，识别为'苏北军的一些爱国将士...'
注意这个SV分数，越高说明说话人匹配越准确

最后一条：SV分数达到0.904，非常精准！"
```

#### 【结果展示】(1 分钟)

```
"让我们看一下详细的统计结果... (运行 show_results.py)

系统共识别出5个分段：
- 2个无重叠分段
- 2个OSD检测分段
- 1个全分离分段

平均SV分数0.746，表现优秀。

关键性能指标：
RTF仅0.12，也就是说处理1秒音频只需0.12秒，
达到了8.3倍实时速度，完全满足实时应用需求。

演示完毕，感谢观看！"
```

---

## 🎯 关键展示点

### 1. 视觉冲击

- ✅ 波形对比图：直观展示混合 vs 目标
- ✅ 实时输出：逐条显示识别结果
- ✅ 颜色高亮：SV 分数用绿/黄/红标识

### 2. 技术亮点

- ✅ 多模块协同（OSD + 分离 + SV + ASR）
- ✅ 高 SV 分数（最高 0.904，平均 0.746）
- ✅ 实时性能（RTF=0.12x，8.3 倍速）

### 3. 结果可信度

- ✅ 分段类型标识（clean/overlap/full_separation）
- ✅ 时间戳精确到毫秒
- ✅ 完整的性能统计

---

## 📦 依赖安装

演示工具需要额外的可视化库：

```bash
pip install matplotlib librosa rich
```

---

## 📁 输出文件结构

```
demo_video/
├── demo_single.sh              # 演示运行脚本
├── visualize_audio.py          # 音频可视化工具
├── show_results.py             # 结果美化展示
├── README.md                   # 本文件
├── visuals/                    # 可视化输出
│   ├── s1_comparison.png
│   ├── s1_mix_waveform.png
│   └── ...
└── demo_output/                # 演示结果
    └── 2026-01-05_xx-xx-xx/
        ├── batch_report.txt
        ├── batch_results.json
        └── all_segments.jsonl
```

---

## 💡 提示

1. **选择 s1 样本**：该样本效果最好（SV 最高 0.904），适合演示
2. **调整延迟**：`show_results.py --delay 1.0` 增加展示间隔
3. **录制前测试**：先完整跑一遍确保流程顺畅
4. **准备备用方案**：如果实时处理卡顿，可以预先录制再配音

---

## 🎬 开始录制！

```bash
# 一键生成所有素材
cd demo_video
python visualize_audio.py --sample s1 --type all

# 录制时运行
bash demo_single.sh

# 展示结果（等处理完成后）
python show_results.py
```
