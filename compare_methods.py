#!/usr/bin/env python3
"""
å¯¹æ¯”éªŒè¯è„šæœ¬ï¼šOSD-based vs Direct Separation

ç”¨äºåœ¨ç°æœ‰æµ‹è¯•æ•°æ®ä¸ŠéªŒè¯ä¸¤ç§æ–¹æ³•çš„æ•ˆæœå·®å¼‚ã€‚
"""
import json
import csv
from pathlib import Path
from typing import Dict, List, Tuple
import numpy as np


def load_comparison_data(analysis_file: str) -> List[Dict]:
    """ä»ç°æœ‰å¯¹æ¯”åˆ†ææ–‡ä»¶ä¸­æå–æ•°æ®"""
    data = []

    # ä» comparison_analysis.md ä¸­æ‰‹åŠ¨æå–çš„æ•°æ®ï¼ˆå¯é€‰è‡ªåŠ¨è§£æï¼‰
    # ä¸ºäº†æ¼”ç¤ºï¼Œè¿™é‡Œä½¿ç”¨é¢„å®šä¹‰çš„æ•°æ®
    samples = [
        {
            "id": "s1",
            "reference": "ç‹è‹±æ±‰è¢«æªæ¯™åï¼Œéƒ¨åˆ†é—å­½æ·±è—èµ·æ¥ï¼Œå‡ æ¬¡å›´æ•å‡æœªæŠ“è·ã€‚",
            "osd_stitching": 0.0,
            "osd_audio_cat": 0.0,
            "direct_separation": 0.5051,
        },
        {
            "id": "s2",
            "reference": "ç¬¬äºŒï¼Œè¦æŠŠç»æµæ‰‹æ®µå’Œæ°å½“çš„è¡Œæ”¿æ‰‹æ®µç»“åˆèµ·æ¥åŠ ä»¥è¿ç”¨ï¼Œç‰¹åˆ«è¦æ³¨æ„è¿ç”¨å¥½ç»æµæ‰‹æ®µã€‚",
            "osd_stitching": 0.0,
            "osd_audio_cat": 0.0,
            "direct_separation": 0.7169,
        },
        {
            "id": "s3",
            "reference": "æ•£æ–‡æ˜¯é¢‡å…·é­…åŠ›çš„ä¸€ç§æ–‡ä½“ï¼Œå› å…¶åœ¨è¯­è¨€æ–‡é‡‡ä¸æ„æ€å’Œæ„å¢ƒä¸Šçš„è®²æ±‚ï¼Œæ•£æ–‡åˆæœ‰ç¾æ–‡ä¹‹ç§°ã€‚",
            "osd_stitching": 0.0,
            "osd_audio_cat": 0.0,
            "direct_separation": 0.7276,
        },
        {
            "id": "s4",
            "reference": "å’Œé›ªä¸€æ ·ï¼Œæœ‰æ— æ•°æ— å£°çš„æ¿€æƒ…ï¼Œä¸ä¸–ç•Œæµ‘ç„¶ä¸€ä½“ï¼Œæ›¾ç»å†å²å…»æ´»ï¼Œåˆå­•è‚²ç¾ä¸½çš„æ˜å¤©ã€‚",
            "osd_stitching": 0.86,
            "osd_audio_cat": 0.86,
            "direct_separation": 0.8605,
        },
        {
            "id": "s5",
            "reference": "ä»–èµ°åˆ°å±‹è§’è„¸ç›†æ¶æ—ï¼ŒæŠŠè„¸ç›†å’£å½“ä¸€å£°æ‰”åœ¨ä¸€æ‘è„¸ç›†ä¸Šã€‚",
            "osd_stitching": 0.0,
            "osd_audio_cat": 0.0,
            "direct_separation": 0.5718,
        },
        {
            "id": "s6",
            "reference": "æ­¤å¤–ï¼Œä¸æ±½è½¦ç›¸å…³çš„éè¿è¾“ä»ä¸šäººå‘˜ä¹Ÿè¾¾2ä¸‡å¤šäººï¼Œå¹´å¯å¢æ”¶5000ä¸‡å…ƒä»¥ä¸Šã€‚",
            "osd_stitching": 0.0,
            "osd_audio_cat": 0.0,
            "direct_separation": 0.6103,
        },
        {
            "id": "s7",
            "reference": "åŠ›ç”Ÿç»è¥çš„ä½“è‚²ç”¨å“åŒ…ç½—ä¸‡è±¡å°åˆ°æ¸¸æ³³è€³å¡ã€å¤§è‡´å¤§å‹ä½“è‚²å™¨æ¢°ï¼Œå¯ä»¥è¯´åº”æœ‰å°½æœ‰ã€‚",
            "osd_stitching": 0.0,
            "osd_audio_cat": 0.0,
            "direct_separation": 0.6946,
        },
        {
            "id": "s8",
            "reference": "ä½ è¦æˆ‘æœ‰äº†çˆ±æƒ…æ‰ç»“å©šï¼Œå¯æ˜¯æˆ‘å°±æ²¡æœ‰å•Šï¼Œä¹Ÿæ‰¾ä¸åˆ°å•Šï¼Œå«æˆ‘æ€ä¹ˆåŠï¼Ÿ",
            "osd_stitching": 0.0,
            "osd_audio_cat": 0.0,
            "direct_separation": 0.6587,
        },
        {
            "id": "s9",
            "reference": "è¶Šå¾€æ£®æ—é‡Œèµ°ï¼Œå…‰è‰²è¶Šæ¥è¶Šæš—ï¼Œè¿œè¿œä¼ æ¥ç†ŠçŒ«ç­‰äººçš„ç›¸äº’å‘¼å”¤ï¼Œä»–ä¹Ÿå‘µå‘µåœ°å¤§å£°ä½œç­”ã€‚",
            "osd_stitching": 0.0,
            "osd_audio_cat": 0.0,
            "direct_separation": 0.5014,
        },
        {
            "id": "s10",
            "reference": "ç‹ä¸€æ±‰è¢«æªæ¯™åï¼Œéƒ¨åˆ†é±¼å­½æ·±è—èµ·æ¥ï¼Œå‡ æ¬¡å›´æ•ï¼Œå‡æœªæŠ“è·ã€‚",
            "osd_stitching": 0.0,
            "osd_audio_cat": 0.0,
            "direct_separation": 0.3470,
        },
    ]

    return samples


def compute_metrics(samples: List[Dict]) -> Dict:
    """è®¡ç®—å„ç§æŒ‡æ ‡"""

    metrics = {
        "samples_count": len(samples),
        "osd_method": {
            "mean_score": 0.0,
            "median_score": 0.0,
            "max_score": 0.0,
            "min_score": 0.0,
            "std_score": 0.0,
            "zero_score_count": 0,
            "nonzero_count": 0,
        },
        "direct_method": {
            "mean_score": 0.0,
            "median_score": 0.0,
            "max_score": 0.0,
            "min_score": 0.0,
            "std_score": 0.0,
            "zero_score_count": 0,
            "nonzero_count": 0,
        },
        "comparison": {
            "direct_wins": 0,
            "osd_wins": 0,
            "ties": 0,
            "direct_better_rate": 0.0,
            "average_improvement": 0.0,
            "max_improvement": 0.0,
            "min_improvement": 0.0,
        },
    }

    # æ”¶é›†åˆ†æ•°
    osd_scores = []
    direct_scores = []
    improvements = []

    for sample in samples:
        # ä½¿ç”¨å¹³å‡OSDåˆ†æ•°ï¼ˆä¸¤ç§æ–¹æ³•éƒ½æ˜¯0ï¼‰
        osd_score = (sample["osd_stitching"] + sample["osd_audio_cat"]) / 2
        direct_score = sample["direct_separation"]

        osd_scores.append(osd_score)
        direct_scores.append(direct_score)
        improvements.append(direct_score - osd_score)

        # è®¡æ•°
        if osd_score == 0:
            metrics["osd_method"]["zero_score_count"] += 1
        else:
            metrics["osd_method"]["nonzero_count"] += 1

        if direct_score == 0:
            metrics["direct_method"]["zero_score_count"] += 1
        else:
            metrics["direct_method"]["nonzero_count"] += 1

        # å¯¹æ¯”
        if direct_score > osd_score:
            metrics["comparison"]["direct_wins"] += 1
        elif direct_score < osd_score:
            metrics["comparison"]["osd_wins"] += 1
        else:
            metrics["comparison"]["ties"] += 1

    # è®¡ç®—ç»Ÿè®¡é‡
    osd_array = np.array(osd_scores)
    direct_array = np.array(direct_scores)
    improvements_array = np.array(improvements)

    metrics["osd_method"]["mean_score"] = round(float(np.mean(osd_array)), 4)
    metrics["osd_method"]["median_score"] = round(float(np.median(osd_array)), 4)
    metrics["osd_method"]["max_score"] = round(float(np.max(osd_array)), 4)
    metrics["osd_method"]["min_score"] = round(float(np.min(osd_array)), 4)
    metrics["osd_method"]["std_score"] = round(float(np.std(osd_array)), 4)

    metrics["direct_method"]["mean_score"] = round(float(np.mean(direct_array)), 4)
    metrics["direct_method"]["median_score"] = round(float(np.median(direct_array)), 4)
    metrics["direct_method"]["max_score"] = round(float(np.max(direct_array)), 4)
    metrics["direct_method"]["min_score"] = round(float(np.min(direct_array)), 4)
    metrics["direct_method"]["std_score"] = round(float(np.std(direct_array)), 4)

    metrics["comparison"]["direct_better_rate"] = round(
        metrics["comparison"]["direct_wins"] / len(samples), 4
    )
    metrics["comparison"]["average_improvement"] = round(
        float(np.mean(improvements_array)), 4
    )
    metrics["comparison"]["max_improvement"] = round(
        float(np.max(improvements_array)), 4
    )
    metrics["comparison"]["min_improvement"] = round(
        float(np.min(improvements_array)), 4
    )

    return metrics


def generate_report(samples: List[Dict], metrics: Dict, output_file: str = None):
    """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""

    print("\n" + "=" * 80)
    print("OSD-based vs Direct Separation å¯¹æ¯”åˆ†ææŠ¥å‘Š".center(80))
    print("=" * 80 + "\n")

    # æ ·æœ¬çº§å¯¹æ¯”è¡¨
    print("ã€æ ·æœ¬çº§å¯¹æ¯”ã€‘")
    print("-" * 120)
    print(
        f"{'ID':<5} {'OSD (avg)':<12} {'Direct':<12} {'Improvement':<15} {'Winner':<10}"
    )
    print("-" * 120)

    for sample in samples:
        osd_avg = (sample["osd_stitching"] + sample["osd_audio_cat"]) / 2
        direct = sample["direct_separation"]
        improvement = direct - osd_avg

        if direct > osd_avg:
            winner = "âœ… Direct"
        elif direct < osd_avg:
            winner = "âŒ OSD"
        else:
            winner = "â¡ï¸  Tie"

        print(
            f"{sample['id']:<5} {osd_avg:<12.4f} {direct:<12.4f} {improvement:+<14.4f} {winner:<10}"
        )

    print("-" * 120 + "\n")

    # èšåˆæŒ‡æ ‡
    print("ã€èšåˆæŒ‡æ ‡ã€‘")
    print("-" * 80)
    print(f"{'æŒ‡æ ‡':<30} {'OSDæ–¹æ³•':<20} {'Directæ–¹æ³•':<20}")
    print("-" * 80)

    osd = metrics["osd_method"]
    direct = metrics["direct_method"]

    print(
        f"{'å¹³å‡åˆ†æ•° (Mean)':<30} {osd['mean_score']:<20.4f} {direct['mean_score']:<20.4f}"
    )
    print(
        f"{'ä¸­ä½æ•° (Median)':<30} {osd['median_score']:<20.4f} {direct['median_score']:<20.4f}"
    )
    print(
        f"{'æœ€é«˜åˆ† (Max)':<30} {osd['max_score']:<20.4f} {direct['max_score']:<20.4f}"
    )
    print(
        f"{'æœ€ä½åˆ† (Min)':<30} {osd['min_score']:<20.4f} {direct['min_score']:<20.4f}"
    )
    print(
        f"{'æ ‡å‡†å·® (Std)':<30} {osd['std_score']:<20.4f} {direct['std_score']:<20.4f}"
    )
    print(
        f"{'é›¶åˆ†æ ·æœ¬æ•°':<30} {osd['zero_score_count']:<20} {direct['zero_score_count']:<20}"
    )
    print(
        f"{'éé›¶åˆ†æ ·æœ¬æ•°':<30} {osd['nonzero_count']:<20} {direct['nonzero_count']:<20}"
    )

    print("-" * 80 + "\n")

    # å¯¹æ¯”ç»“æœ
    print("ã€å¯¹æ¯”ç»“æœã€‘")
    print("-" * 80)
    cmp = metrics["comparison"]
    total = cmp["direct_wins"] + cmp["osd_wins"] + cmp["ties"]

    print(f"{'æ€»æ ·æœ¬æ•°':<30} {total}")
    print(
        f"{'Directæ–¹æ³•èƒœåˆ©':<30} {cmp['direct_wins']} / {total} ({cmp['direct_better_rate']*100:.1f}%)"
    )
    print(
        f"{'OSDæ–¹æ³•èƒœåˆ©':<30} {cmp['osd_wins']} / {total} ({cmp['osd_wins']/total*100:.1f}%)"
    )
    print(f"{'å¹³å±€':<30} {cmp['ties']} / {total} ({cmp['ties']/total*100:.1f}%)")
    print(f"{'å¹³å‡æ”¹è¿›å€¼':<30} {cmp['average_improvement']:+.4f}")
    print(f"{'æœ€å¤§æ”¹è¿›':<30} {cmp['max_improvement']:+.4f}")
    print(f"{'æœ€å°æ”¹è¿›':<30} {cmp['min_improvement']:+.4f}")

    print("-" * 80 + "\n")

    # ç»“è®º
    print("ã€ç»“è®ºã€‘")
    print("-" * 80)
    if cmp["direct_better_rate"] > 0.6:
        print("âœ… Direct Separationæ–¹æ³•åœ¨æœ¬æ•°æ®é›†ä¸Šè¡¨ç°æ˜æ˜¾ä¼˜äºOSD-basedæ–¹æ³•")
        print(f"   - èƒœç‡ï¼š{cmp['direct_better_rate']*100:.1f}%")
        print(f"   - å¹³å‡æ”¹è¿›ï¼š{cmp['average_improvement']:+.4f} åˆ†")
    else:
        print("âš ï¸  ä¸¤ç§æ–¹æ³•è¡¨ç°æ¥è¿‘ï¼Œéœ€è¦æ›´å¤šæ•°æ®éªŒè¯")

    print("\nğŸ“Š å»ºè®®ï¼š")
    if cmp["direct_better_rate"] > 0.6:
        print("   1. é‡‡ç”¨ Direct Separation ä½œä¸ºä¸»å¤„ç†è·¯å¾„")
        print("   2. ä¿ç•™ OSD ä½œä¸ºå¯é€‰ç›‘æ§å·¥å…·ï¼ˆä¸æ§åˆ¶å¤„ç†æµç¨‹ï¼‰")
        print("   3. ç®€åŒ– streaming_overlap3_core.pyï¼Œç§»é™¤å†—ä½™çš„ OSD-based åˆ†æ”¯")

    print("\n" + "=" * 80 + "\n")

    # ä¿å­˜ç»“æœ
    if output_file:
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "samples": samples,
                    "metrics": metrics,
                },
                f,
                ensure_ascii=False,
                indent=2,
            )
        print(f"ğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°ï¼š{output_file}")


def main():
    import argparse

    parser = argparse.ArgumentParser(description="OSD vs Direct Separation å¯¹æ¯”åˆ†æ")
    parser.add_argument(
        "--analysis-file",
        default="test_overlap/comparison_analysis.md",
        help="è¾“å…¥çš„å¯¹æ¯”åˆ†ææ–‡ä»¶",
    )
    parser.add_argument(
        "--output",
        default="comparison_metrics.json",
        help="è¾“å‡ºç»“æœæ–‡ä»¶ï¼ˆJSONæ ¼å¼ï¼‰",
    )

    args = parser.parse_args()

    # åŠ è½½æ•°æ®
    samples = load_comparison_data(args.analysis_file)

    # è®¡ç®—æŒ‡æ ‡
    metrics = compute_metrics(samples)

    # ç”ŸæˆæŠ¥å‘Š
    generate_report(samples, metrics, args.output)


if __name__ == "__main__":
    main()
